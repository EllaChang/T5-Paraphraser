adam_epsilon: 1.0e-08
data_dir: paraphrase_data
early_stop_callback: false
eval_batch_size: 6
fp_16: false
gradient_accumulation_steps: 16
learning_rate: 0.0003
max_grad_norm: 1.0
max_seq_length: 256
model_name_or_path: t5-base
n_gpu: 1
num_train_epochs: 2
opt_level: O1
output_dir: t5_paraphrase
seed: 42
tokenizer_name_or_path: t5-base
train_batch_size: 6
warmup_steps: 0
weight_decay: 0.0
